kubeadm安装kubenetes
https://blog.csdn.net/wangshuminjava/article/details/100090945

#kubeadm设计
https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.10.md

kubernets v1.15.4   //v1.13+ 支持国内aliyun容器镜像拉去

//k8s的部署形式
kubeadm部署
借助于工具kubespray,kopts部署
ansible,基于roles实现部署

kubeadm方式部署，k8s可以把k8s自身的大部分应用管控起来，即运行于pod上，但是kubelet和docker不能这样实现自托管，这两个主机运行为守护进程，因此，只需要在所有主机都安装kubelet和docker,构建k8s集群。
相当于是自举。etcd也是托管于pod上运行，使用kubeadm进行部署，安装过程相对简单。这些主件的pod一般为静态pod(不属于k8s管理)，也可以运行为自托管的pod.每个主机都要运行flannel这个主件，可以运行为pod。flannel为动态pod
https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.10.md
通过kubeadm init完成集群master节点的初始化，用kubeadm join加入集群


手动配置主节点和node都主要组件运行为系统级的守护进程，每一步都需要手动处理，如证书和配置过程都是用手动配置的。
另外，这种方式在github上有playbook自动化实现

a).master:安装apiserver,scheduler,controller-manager,etcd,flanel
b).node:安装kublet,kub-proxy,docker(container engine),flannel,需要多个节点
c).etcd:安装etcd存储服务器，建议配置为高可用
https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md#downloads-for-v1112
下载相关的安装包，注意，master或者node都是要安装server端的包。client是交互时使用，也需要安装，不建议使用这种方式安装，难度较高。


本文仅介绍使用kubeadm实现k8s集群安装
//环境
01、准备工作
关闭防火墙和selinux/hosts/ntp
禁用swap
服务器配置，至少2核2G
swapoff -a  && /etc/fstab

#所有节点
cat >/etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
vm.swappiness=0
EOF

sysctl --system  &&  sysctl -p

#hostname 只能有-/.特殊字符，防止coredns解析不了
#ipvs相关(忽略)   //service工作 lvs模式效率较高，默认iptables
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

chmod 755 /etc/sysconfig/modules/ipvs.modules
bash /etc/sysconfig/modules/ipvs.modules
lsmod | grep -e ip_vs -e nf_conntrack_ipv4

yum install -y ipset ipvsadm   //各个节点，为了查看ipvs代理规则


02、配置原件源及安装
cd /etc/yum.repos.d/
wget -O /etc/yum.repos.d/docker-ce.repo  https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

cat>/etc/yum.repos.d/kubrenetes.repo<<EOF
[kubernetes]
name=Kubernetes Repo
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
EOF

yum clean all && yum makecache

#下载服务
#查看docker-ce版本
yum list docker-ce.x86_64  --showduplicates |sort -r

#k8s1.14最高支持18.09版本的docker
yum install -y --setopt=obsoletes=0 docker-ce-18.09.6-3.el7

mkdir -p /etc/docker
vim /etc/docker/daemon.json   //配置docker加速
{
 "registry-mirrors": ["https://eyg9yi6d.mirror.aliyuncs.com"]
}
systemctl enable docker.service

yum list kubeadm  --showduplicates |sort -r
kubeadm.x86_64                       1.13.0-0                         kubernetes
yum install  kubelet-1.13.0 kubeadm-1.13.0 kubectl-1.13.0 -y 

yum install  kubelet-1.15.4 kubeadm-1.15.4 kubectl-1.15.4 -y   //版本要保持一致性
yum install  kubelet-1.15.4 kubeadm-1.15.4 -y   //nodes 节点可以不安装kubectl

###安装结果可以看出还安装了cri-tools, kubernetes-cni, socat三个依赖： 
官方从Kubernetes 1.9开始就将cni依赖升级到了0.6.0版本，在当前1.12中仍然是这个版本 
socat是kubelet的依赖 
cri-tools是CRI(Container Runtime Interface)容器运行时接口的命令行工具

yum -y install ipvsadm ipset
注意：kubeadm/kubectl/kubelet/kubernetes 版本一致

[root@c-3-50 ~]# rpm -ql kubelet
/etc/kubernetes/manifests
/etc/sysconfig/kubelet
/usr/bin/kubelet
/usr/lib/systemd/system/kubelet.service

[root@c-3-50 ~]# cat /etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS="--fail-swap-on=false"
KUBE_PROXY_MODE=ipvs

tee >/etc/sysconfig/kubelet <<EOF
KUBELET_EXTRA_ARGS="--fail-swap-on=false"
KUBE_PROXY_MODE=ipvs
EOF

systemctl daemon-reload && systemctl restart kubelet

or
tee /etc/default/kubelet <<-'EOF'
KUBELET_EXTRA_ARGS="--fail-swap-on=false"
KUBE_PROXY_MODE=ipvs
EOF

//设置cgroup driver
kubeadm会读取/etc/systemd/system/kubelet.service.d/10-kubeadm.conf文件的配置信息
更改cgroup的驱动,这里的驱动要要docker一致因此用docker info| grep Cgroup命令查看docker的驱动类型
如这里查到的结果为 cgroupfs.因此修改kubeadm的配置文件如下
vim  /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"
启动kubelet
systemctl start kubelet
注意，此时启动会出现报错，查看/var/log/messages的日志
tail -f /var/log/messages

systemctl daemon-reload && systemctl restart kubelet

master节点执行操作
#启动服务
systemctl   restart docker
systemctl   enable docker
systemctl   enable kubelet && systemctl restart kubelet


[root@c-3-104 ~]# kubeadm version -o yaml
clientVersion:
  buildDate: "2018-12-03T21:02:01Z"
  compiler: gc
  gitCommit: ddf47ac13c1a9483ea035a79cd7c10005ff21a6d
  gitTreeState: clean
  gitVersion: v1.13.0
  goVersion: go1.11.2
  major: "1"
  minor: "13"
  platform: linux/amd64


#kubeadm初始化
[root@c-3-104 ~]# kubeadm config images list --kubernetes-version=1.13.0   //默认需要用到的images
k8s.gcr.io/kube-apiserver:v1.13.0
k8s.gcr.io/kube-controller-manager:v1.13.0
k8s.gcr.io/kube-scheduler:v1.13.0
k8s.gcr.io/kube-proxy:v1.13.0
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.2.24
k8s.gcr.io/coredns:1.2.6

kubeadm init \
  --kubernetes-version=v1.15.4 \
  --image-repository registry.aliyuncs.com/google_containers \
  --pod-network-cidr=10.10.0.0/16 \
  --service-cidr=10.20.0.0/16 \
  --ignore-preflight-errors=Swap

kubeadm init \
  --kubernetes-version=v1.13.0 \
  --image-repository registry.aliyuncs.com/google_containers \
  --pod-network-cidr=10.10.0.0/16 \
  --service-cidr=10.20.0.0/16 \
  --ignore-preflight-errors=Swap

[apiclient] All control plane components are healthy after 115.009951 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.15" in namespace kube-system with the configuration for the kub
elets in the cluster[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node c-3-50 as control-plane by adding the label "node-role.kubernetes.io/maste
r=''"[mark-control-plane] Marking the node c-3-50 as control-plane by adding the taints [node-role.kubernetes.io/mast
er:NoSchedule][bootstrap-token] Using token: mrhnx5.mi7em6ziutdn6gdn
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get lo
ng term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a No
de Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cl
uster[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: 
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.3.50:6443 --token mrhnx5.mi7em6ziutdn6gdn \
    --discovery-token-ca-cert-hash sha256:1dba1de642dcc787ce95e602b356acc95de62ee4ea330cb8727241ea1b602582 \
    --ignore-preflight-errors=Swap

//init流程
[kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”
[certificates]生成相关的各种证书
[kubeconfig]生成相关的kubeconfig文件
[bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到


#记录初始化后的kubeadm join 信息
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 
kubectl get cs   //获取k8s组件状态
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health": "true"}

//安装异常重置服务，重新安装
kubeadm reset
ifconfig cni0 down
ip link delete cni0
ifconfig flannel.1 down
ip link delete flannel.1
rm -rf /var/lib/cni/


#安装flannel网络（也可以安装其他网络）
kubectl  apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kube-flannel.yml  //修改网卡--iface=<iface-name> ,默认eth0

[root@c-3-50 ~]# kubectl get pods -A
NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE
kube-system   coredns-bccdc95cf-2g8z5          0/1     Pending   0          5m13s
kube-system   coredns-bccdc95cf-q9r9k          0/1     Pending   0          5m13s
kube-system   etcd-c-3-50                      1/1     Running   0          4m18s
kube-system   kube-apiserver-c-3-50            1/1     Running   0          4m35s
kube-system   kube-controller-manager-c-3-50   1/1     Running   0          4m35s
kube-system   kube-flannel-ds-amd64-2sm64      1/1     Running   0          29s
kube-system   kube-proxy-s6svm                 1/1     Running   0          5m12s
kube-system   kube-scheduler-c-3-50            1/1     Running   0          4m16s

#安装calico网络
wget https://docs.projectcalico.org/v3.7/manifests/calico.yaml
#更换网段
sed -i "s#192.168.0.0/16#10.10.0.0/16#g" calico.yaml   //pod_network
#导入
kubectl  apply -f calico.yaml

//curl测试
kubectl run -it  curl --image=radial/busyboxplus:curl
nslookup kubernetes.default


node节点操作
#启动服务
systemctl   restart docker
systemctl   enable docker
 
#执行master上显示的kubeadm join命令 (类似如下)
kubeadm join 172.31.250.160:6443 --token fx3ua3.4cxlvfnbrhiwpnj8     --discovery-token-ca-cert-hash sha256:1ac1ece9c7b61fb88208680ba9e864d3a496a81be4bc2212833327b14d0991bf

在master端使用kubectl get node 查看即可
[root@k8s-m ~]# kubectl  get node
NAME    STATUS   ROLES    AGE     VERSION
k8s-m   Ready    master   12m     v1.14.2
node    Ready    <none>   9m22s   v1.14.2

[root@c-3-50 ~]# kubectl get nodes   //master节点上操作
NAME     STATUS     ROLES    AGE     VERSION
c-3-50   Ready      master   20m     v1.15.4
c-3-51   Ready      <none>   2m38s   v1.15.4
c-3-52   NotReady   <none>   111s    v1.15.4   //nodes 节点会从master节点上下载需要的镜像，稍等即可

kubectl get pods -n kube-system -o wide  //查看kube-system 命名空间的信息
kubectl describe pod kube-proxy-7rms5 --namaespace=kube-system   //查看pod的信息

##http://www.ik8s.io/  代理下载
/usr/lib/systemd/system/docker.server
Environment="HTTPS_PROXY=http://www.ik8s.io:10080" 
Environment="NO_PROXY=127.0.0.0/8,192.168.0.0/16"

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1

//特殊的images拉去方式
vim pullimages.sh
#!/bin/bash
images=(kube-proxy-amd64:v1.11.2 kube-scheduler-amd64:v1.11.2 kube-controller-manager-amd64:v1.11.2 kube-apiserver-amd64:v1.11.2
etcd-amd64:3.2.18 coredns:1.1.3 pause:3.1 )
for imageName in ${images[@]} ; do
docker pull anjia0532/google-containers.$imageName
docker tag anjia0532/google-containers.$imageName k8s.gcr.io/$imageName
docker rmi anjia0532/google-containers.$imageName
done

###ipvs
[root@c-3-54 ~]# kubectl logs -n kube-system  pod/kube-proxy-q5fmk   //查看kube-proxy启动代理的方式
W0406 15:11:11.338174       1 server_others.go:249] Flag proxy-mode="" unknown, assuming iptables proxy  //proxy-mode=""
I0406 15:11:11.346811       1 server_others.go:143] Using iptables Proxier.
I0406 15:11:11.349519       1 server.go:534] Version: v1.15.4
I0406 15:11:11.364714       1 conntrack.go:100] Set sysctl 'net/netfilter/nf_conntrack_max' to 131072
I0406 15:11:11.364766       1 conntrack.go:52] Setting nf_conntrack_max to 131072
I0406 15:11:11.366975       1 conntrack.go:83] Setting conntrack hashsize to 32768
I0406 15:11:11.371127       1 conntrack.go:100] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400
I0406 15:11:11.371244       1 conntrack.go:100] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600
I0406 15:11:11.371489       1 config.go:187] Starting service config controller
I0406 15:11:11.371530       1 controller_utils.go:1029] Waiting for caches to sync for service config controller
I0406 15:11:11.371929       1 config.go:96] Starting endpoints config controller
I0406 15:11:11.371947       1 controller_utils.go:1029] Waiting for caches to sync for endpoints config controller
I0406 15:11:11.471914       1 controller_utils.go:1036] Caches are synced for service config controller
I0406 15:11:11.472272       1 controller_utils.go:1036] Caches are synced for endpoints config controller

[root@k8s-master ~]# kubectl edit cm kube-proxy -n kube-system   //kube-proxy修改控制管理配置
...
ipvs:
      excludeCIDRs: null
      minSyncPeriod: 0s
      scheduler: ""
      strictARP: false
      syncPeriod: 30s
    kind: KubeProxyConfiguration
    metricsBindAddress: 127.0.0.1:10249
    mode: "ipvs"   //设置ipvs
 
//重启kube-proxy pod
kubectl -n kube-system get pods  | grep kube-proxy |awk '{system("kubectl -n kube-system delete pod "$1" ")}'

//查看kube-proxy pod 启动日志
[root@c-3-54 ~]# kubectl get pod -n kube-system |grep kube-proxy |awk '{system("kubectl logs pod/"$1" -n kube-system")}'
I0406 15:19:02.600751       1 server_others.go:170] Using ipvs Proxier.
W0406 15:19:02.603497       1 proxier.go:401] IPVS scheduler not specified, use rr by default
I0406 15:19:02.605293       1 server.go:534] Version: v1.15.4
I0406 15:19:02.614387       1 conntrack.go:52] Setting nf_conntrack_max to 131072
I0406 15:19:02.617550       1 config.go:96] Starting endpoints config controller
I0406 15:19:02.617597       1 controller_utils.go:1029] Waiting for caches to sync for endpoints config controller
I0406 15:19:02.617631       1 config.go:187] Starting service config controller
I0406 15:19:02.617641       1 controller_utils.go:1029] Waiting for caches to sync for service config controller
I0406 15:19:02.718233       1 controller_utils.go:1036] Caches are synced for service config controller
I0406 15:19:02.718244       1 controller_utils.go:1036] Caches are synced for endpoints config controller

[root@c-3-54 ~]# kubectl get pod -n kube-system 

[root@c-3-54 ~]# kubectl logs -n kube-system  pod/kube-proxy-bmbqs 
I0406 15:19:02.600751       1 server_others.go:170] Using ipvs Proxier.
W0406 15:19:02.603497       1 proxier.go:401] IPVS scheduler not specified, use rr by default  //默认rr
I0406 15:19:02.605293       1 server.go:534] Version: v1.15.4
I0406 15:19:02.614387       1 conntrack.go:52] Setting nf_conntrack_max to 131072
I0406 15:19:02.617550       1 config.go:96] Starting endpoints config controller
I0406 15:19:02.617597       1 controller_utils.go:1029] Waiting for caches to sync for endpoints config controller
I0406 15:19:02.617631       1 config.go:187] Starting service config controller
I0406 15:19:02.617641       1 controller_utils.go:1029] Waiting for caches to sync for service config controller
I0406 15:19:02.718233       1 controller_utils.go:1036] Caches are synced for service config controller
I0406 15:19:02.718244       1 controller_utils.go:1036] Caches are synced for endpoints config controller

注意：在不能ping通pod中容器ip请重启对应服务器，或者清理iptables规则
iptables -P INPUT ACCEPT
iptables -P OUTPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -F


###troubeshoot 集群的POD内不能访问clusterIP和service
https://blog.51cto.com/13641616/2442005

//单机kubeadm 需要去污点
使用kubeadm部署的kubernetes集群，其master节点默认拒绝将pod调度运行于其上的
官方的术语就是：master默认被赋予了一个或者多个"污点（taints"，"污点”的作用是让该节点拒绝将pod调度运行于其上。

那么存在某些情况，比如想让master也成为工作节点可以调度pod运行怎么办呢？
两种方式：
①去掉“污点”（taints）【生产环境不推荐】；
②让pod能够容忍（tolerations）该节点上的“污点”


[root@c-3-54 ~]# kubectl describe node c-3-54 |grep -i taint
Taints:             node-role.kubernetes.io/master:NoSchedule   //默认master NoSchedule

kubectl taint node [node] key=value[effect]   
     其中[effect] 可取值: [ NoSchedule | PreferNoSchedule | NoExecute ]
      NoSchedule: 一定不能被调度
      PreferNoSchedule: 尽量不要调度
      NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod

[root@c-3-54 ~]# kubectl get nodes
NAME     STATUS   ROLES    AGE   VERSION
c-3-54   Ready    master   22m   v1.15.4


kubectl taint node node1 key1:NoSchedule-       # 这里的key可以不用指定value
kubectl taint node node1 key1:NoExecute-
kubectl taint node node1 key1-                  # 删除指定key所有的effect
kubectl taint node node1 key2:NoSchedule-


kubectl taint nodes node1 node-role.kubernetes.io/master-   //去污点
[root@c-3-54 ~]# kubectl taint nodes c-3-54 node-role.kubernetes.io/master:NoSchedule-   //移除策略
[root@c-3-54 ~]# kubectl describe node c-3-54 |grep -i taint
Taints:             <none>

[root@server76 ~]# kubectl get cs    //集群组件状态
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   
scheduler            Healthy   ok                   
etcd-0               Healthy   {"health": "true"}

[root@c-3-104 ~]# kubectl get nodes
NAME      STATUS     ROLES    AGE     VERSION
c-3-104   NotReady   master   5m10s   v1.13.0

[root@server76 docker]# kubectl get pods -n kube-system
[root@server76 docker]# kubectl get ns     //namespace
 

//kube-proxy开启ipvs
修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: "ipvs"
kubectl edit cm kube-proxy -n kube-system

重启各个节点上的kube-proxy pod
kubectl get pod -n kube-system | grep kube-proxy | awk '{system("kubectl delete pod "$1" -n kube-system")}'

kubectl get pod -n kube-system | grep kube-proxy
kubectl logs kube-proxy-pf55q -n kube-system  //启动日志查看proxy模式

//Helm这个Kubernetes的包管理器
Helm来安装、升级、回滚一个 Kubernetes 应用
https://helm.sh/docs/intro/quickstart/
https://github.com/helm/helm


Helm由客户端命helm令行工具和服务端tiller组成
Helm的安装十分简单下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.12.0版本
https://github.com/helm/helm/releases/tag/v2.12.3

wget https://storage.googleapis.com/kubernetes-helm/helm-v2.12.0-linux-amd64.tar.gz  //google下载不动
wget https://get.helm.sh/helm-v2.12.3-linux-amd64.tar.gz

tar -zxvf helm-v2.12.3-linux-amd64.tar.gz
cd linux-amd64/ && cp helm /usr/bin/

安装helm服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件
确保kubectl工具可以在这台机器上访问apiserver且正常使用   //说明配置是ok的

#kubernetes 从1.6 版本开始加入了RBAC 授权
Kubernetes APIServer开启了RBAC访问控制所以需要创建tiller使用的service account: tiller并分配合适的角色给它
https://helm.sh/docs/using_helm/#role-based-access-control
简单起见直接分配cluster-admin这个集群内置的ClusterRole给它、创建rbac-config.yaml文件
#rbac-config.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system

kubectl create -f rbac-config.yaml

helm部署tiller
[root@c-3-104 ~]# helm -h
The Kubernetes package manager

To begin working with Helm, run the 'helm init' command:

	$ helm init

This will install Tiller to your running Kubernetes cluster.
It will also set up any necessary local configuration.

Common actions from this point include:

- helm search:    search for charts
- helm fetch:     download a chart to your local directory to view
- helm install:   upload the chart to Kubernetes
- helm list:      list releases of charts

Environment:
  $HELM_HOME           set an alternative location for Helm files. By default, these are stored in ~/.helm
  $HELM_HOST           set an alternative Tiller host. The format is host:port
  $HELM_NO_PLUGINS     disable plugins. Set HELM_NO_PLUGINS=1 to disable plugins.
  $TILLER_NAMESPACE    set an alternative Tiller namespace (default "kube-system")
  $KUBECONFIG          set an alternative Kubernetes configuration file (default "~/.kube/config")
  $HELM_TLS_CA_CERT    path to TLS CA certificate used to verify the Helm client and Tiller server certificates (default "$HELM_HOME/ca.pem
")  $HELM_TLS_CERT       path to TLS client certificate file for authenticating to Tiller (default "$HELM_HOME/cert.pem")
  $HELM_TLS_KEY        path to TLS client key file for authenticating to Tiller (default "$HELM_HOME/key.pem")
  $HELM_TLS_VERIFY     enable TLS connection between Helm and Tiller and verify Tiller server certificate (default "false")
  $HELM_TLS_ENABLE     enable TLS connection between Helm and Tiller (default "false")
  $HELM_KEY_PASSPHRASE set HELM_KEY_PASSPHRASE to the passphrase of your PGP private key. If set, you will not be prompted for
                       the passphrase while signing helm charts

Usage:
  helm [command]

Available Commands:
  completion  Generate autocompletions script for the specified shell (bash or zsh)
  create      create a new chart with the given name
  delete      given a release name, delete the release from Kubernetes
  dependency  manage a chart's dependencies
  fetch       download a chart from a repository and (optionally) unpack it in local directory
  get         download a named release
  help        Help about any command
  history     fetch release history
  home        displays the location of HELM_HOME
  init        initialize Helm on both client and server
  inspect     inspect a chart
  install     install a chart archive
  lint        examines a chart for possible issues
  list        list releases
  package     package a chart directory into a chart archive
  plugin      add, list, or remove Helm plugins
  repo        add, list, remove, update, and index chart repositories
  reset       uninstalls Tiller from a cluster
  rollback    roll back a release to a previous revision
  search      search for a keyword in charts
  serve       start a local http web server
  status      displays the status of the named release
  template    locally render templates
  test        test a release
  upgrade     upgrade a release
  verify      verify that a chart at the given path has been signed and is valid
  version     print the client/server version information

Flags:
      --debug                           enable verbose output
  -h, --help                            help for helm
      --home string                     location of your Helm config. Overrides $HELM_HOME (default "/root/.helm")
      --host string                     address of Tiller. Overrides $HELM_HOST
      --kube-context string             name of the kubeconfig context to use
      --kubeconfig string               absolute path to the kubeconfig file to use
      --tiller-connection-timeout int   the duration (in seconds) Helm will wait to establish a connection to tiller (default 300)
      --tiller-namespace string         namespace of Tiller (default "kube-system")

Use "helm [command] --help" for more information about a command.


[root@c-3-104 ~]# helm init --service-account tiller --skip-refresh
Creating /root/.helm 
Creating /root/.helm/repository 
Creating /root/.helm/repository/cache 
Creating /root/.helm/repository/local 
Creating /root/.helm/plugins 
Creating /root/.helm/starters 
Creating /root/.helm/cache/archive 
Creating /root/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Adding local repo with URL: http://127.0.0.1:8879/charts 
$HELM_HOME has been configured at /root/.helm.

Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.

Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.
To prevent this, run `helm init` with the --tiller-tls-verify flag.
For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation
Happy Helming!
[root@c-3-104 ~]# 


#tiller默认被部署在k8s集群中的kube-system这个namespace
[root@c-3-104 ~]# kubectl get pod -n kube-system -l app=helm
NAME                            READY   STATUS             RESTARTS   AGE
tiller-deploy-dbb85cb99-kmmnb   0/1     ImagePullBackOff   0          61s
[root@c-3-104 ~]# 
//镜像拉去失败
注意由于某些原因需要网络可以访问gcr.io和kubernetes-charts.storage.googleapis.com如果无法访问可以通过
helm init --service-account tiller --tiller-image <your-docker-registry>/tiller:v2.11.0 --skip-refresh
使用私有镜像仓库中的tiller镜像
helm init --service-account tiller --tiller-image registry.aliyuncs.com/google_containers/tiller:v2.11.0 --skip-refresh

[root@c-3-104 ~]# docker pull registry.aliyuncs.com/google_containers/tiller:v2.11.0

[root@c-3-104 ~]# helm init --service-account tiller --tiller-image registry.aliyuncs.com/google_containers/tiller:v2.11.0 --skip-refresh
$HELM_HOME has been configured at /root/.helm.
Warning: Tiller is already installed in the cluster.
(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)
Happy Helming!
[root@c-3-104 ~]# rm -rf .helm  //重新在执行

#删除tiller
kubectl delete deployment tiller-deploy --namespace kube-system  //helm reset
https://www.jianshu.com/p/4bd853a8068b

//helm init --service-account tiller --tiller-image registry.aliyuncs.com/google_containers/tiller:v2.11.0 --skip-refresh

helm init --service-account tiller -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.11.0  --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts --skip-refresh
helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1  --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts

helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.3

//error  tiller与helm版本一致
[root@c-3-104 ~]# helm install stable/mysql
Error: incompatible versions client[v2.12.3] server[v2.11.0]

[root@c-3-104 ~]# helm version
Client: &version.Version{SemVer:"v2.12.3", GitCommit:"eecf22f77df5f65c823aacd2dbd30ae6c65f186e", GitTreeState:"clean"}
Error: could not find a ready tiller pod

//helm hub仓库
https://hub.helm.sh/

helm chart repo源镜像地址汇总
#阿里云hub
https://github.com/cloudnativeapp/charts 
https://developer.aliyun.com/hub   //ok

微软
stable http://mirror.azure.cn/kubernetes/charts
incubator http://mirror.azure.cn/kubernetes/charts-incubator
svc-cat http://mirror.azure.cn/kubernetes/svc-catalog-charts

elastic的 elasticsearch
elastic https://helm.elastic.co

helm repo add stable http://mirror.azure.cn/kubernetes/charts

[root@c-3-104 ~]# helm search nginx
$ helm repo update              # Make sure we get the latest list of charts
$ helm install stable/mysql

//It's easy to see what has been released using Helm:
$ helm ls
NAME             VERSION   UPDATED                   STATUS    CHART
smiling-penguin  1         Wed Sep 28 12:59:46 2016  DEPLOYED  mysql-0.1.0
$ helm delete smiling-penguin

helm search mysql
helm inspect stable/mariadb  //查看详细信息
helm install stable/mariadb