# kubernetes 
https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.10.md

#含有kubernets bin/images  
https://dl.k8s.io/v1.10.13/kubernetes-server-linux-amd64.tar.gz

#参考
[link](https://blog.csdn.net/shenhonglei1234/article/details/80803489)

#涉及到的文件
```
https://pan.baidu.com/s/1GV5-PLJ316mE9GwLmqPmnw 
提取码：8frn

https://pan.baidu.com/s/1nV_lOOJhNJpqGBq9heNWug 
提取码: zkfr
```

# 部署规划
- 192.168.3.103 master
- 192.168.3.104 work
- centos7 x64 2c4g
- kubeadm v1.10.12
- flannel
- docker-ce-18.06.3.ce-3.el7

# 01、requirement 
- hosts
- firewalld
- selinux
- swap

#disable swap
```
[root@c-3-103 ~]# swapoff  -a
[root@c-3-103 ~]# vim /etc/fstab
[root@c-3-103 ~]# mount -a
[root@c-3-103 ~]# free -m
              total        used        free      shared  buff/cache   available
Mem:           3773          99        3532          11         140        3466
Swap:             0           0           0
```

# 02、docker
#install docker-ce
[link](https://gitee.com/m0p/k8s-stu/blob/master/00-docker/docker-ce/setup.md)

```
yum install -y docker-ce-18.06.3.ce-3.el7
systemctl enable docker && systemctl start docker 
```

#confirm cgroup-driver
```
[root@c-3-103 ~]# docker info |grep Cgroup
Cgroup Driver: cgroupfs

#docker和kubelet的cgroup driver需要一致，如果docker不是cgroupfs
tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://XXXX.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=cgroupfs"]   //设置cgroupdriver
}
EOF

systemctl daemon-reload && systemctl restart docker
```

# 03、install kubeadm
#kubeadm mirrors
```
cat>/etc/yum.repos.d/kubrenetes.repo<<EOF
[kubernetes]
name=Kubernetes Repo
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
enable=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
EOF

[root@c-3-104 ~]# yum list kubeadm  --showduplicates |sort -r |grep 1.10
```

#master-node
```
yum install -y kubelet-1.10.13 kubeadm-1.10.13 kubectl-1.10.13 kubernetes-cni-0.6.0
```

#work-node
```
yum install -y kubelet-1.10.13 kubeadm-1.10.13 kubernetes-cni-0.6.0
```

>注意：kubeadm/kubectl/kubelet/kubernetes 版本一致

systemctl enable  kubelet

#confirm cgroup-drive
```
#查看kubelet 默认的cgroup-drive
[root@c-3-103 ~]# grep cgroup-drive  /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"

#默认kubelet使用的cgroup-driver=systemd，改为cgroup-driver=cgroupfs 
sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 

#重设kubelet服务，并重启kubelet服务
systemctl daemon-reload && systemctl restart kubelet
```

#导入离线镜像(国外的拉取太困难了)
```
[root@c-3-103 k8s]# docker load -i k8s-images-1.10.tar.gz   //需要的镜像已经打包好了，每个节点上导入一次

[root@c-3-103 k8s]# docker  images
REPOSITORY                                 TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy-amd64                v1.10.1             6e6237849607        2 years ago         97.1MB
k8s.gcr.io/kube-apiserver-amd64            v1.10.1             9df3c00f55e6        2 years ago         225MB
k8s.gcr.io/kube-scheduler-amd64            v1.10.1             ceecd7155649        2 years ago         50.4MB
k8s.gcr.io/kube-controller-manager-amd64   v1.10.1             8401bb3ff261        2 years ago         148MB
k8s.gcr.io/etcd-amd64                      3.1.12              52920ad46f5b        2 years ago         193MB
k8s.gcr.io/kubernetes-dashboard-amd64      v1.8.3              0c60bcf89900        2 years ago         102MB
k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64     1.14.8              c2ce1ffb51ed        2 years ago         41MB
k8s.gcr.io/k8s-dns-sidecar-amd64           1.14.8              6f7f2dc7fab5        2 years ago         42.2MB
k8s.gcr.io/k8s-dns-kube-dns-amd64          1.14.8              80cc5ea4b547        2 years ago         50.5MB
k8s.gcr.io/pause-amd64                     3.1                 da86e6ba6ca1        2 years ago         742kB
quay.io/coreos/flannel                     v0.9.1-amd64        2b736d06ca4c        2 years ago         51.3MB
[root@c-3-103 k8s]# 
```

# 04、init k8s
```
[root@c-3-103 k8s]# kubeadm init --kubernetes-version=v1.10.1 --pod-network-cidr=10.10.0.0/16  --service-cidr=10.20.0.0/16

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

01、config kubectl
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

02、install cni
You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

03、join work into k8s
You can now join any number of machines by running the following on each node
as root:

  kubeadm join 192.168.3.103:6443 --token 4ipjp8.8qmixoik7538pkro --discovery-token-ca-cert-hash  sha256:733202c22d5b2b8381c0e39bbcd258038924284497834a7bc84978d60d7ea78e
```

# 05、 check pods/nodes && install cni
```
[root@c-3-103 k8s]# kubectl get nodes
NAME      STATUS     ROLES     AGE       VERSION
c-3-103   NotReady   master    3m        v1.10.13

[root@c-3-103 k8s]# kubectl get pods --all-namespaces
NAMESPACE     NAME                              READY     STATUS    RESTARTS   AGE
kube-system   etcd-c-3-103                      1/1       Running   0          2m
kube-system   kube-apiserver-c-3-103            1/1       Running   0          2m
kube-system   kube-controller-manager-c-3-103   1/1       Running   0          2m
kube-system   kube-dns-86f4d74b45-798rx         0/3       Pending   0          3m  //需要安装k8s network CNI
kube-system   kube-proxy-7fdhz                  1/1       Running   0          3m
kube-system   kube-scheduler-c-3-103            1/1       Running   0          2m
[root@c-3-103 k8s]# 
```

#network flannel  
k8s支持多种网络方案，flannel，calico，openvswitch  

[root@c-3-103 k8s]# kubectl apply -f kube-flannel.yml
```
  net-conf.json: |
    {
      "Network": "10.10.0.0/16",   //pod-cidr as same
      "Backend": {
        "Type": "vxlan"
      }
    }

```

#check again
```
[root@c-3-103 k8s]# kubectl get pods --all-namespaces
NAMESPACE     NAME                              READY     STATUS    RESTARTS   AGE
kube-system   etcd-c-3-103                      1/1       Running   0          28s
kube-system   kube-apiserver-c-3-103            1/1       Running   0          25s
kube-system   kube-controller-manager-c-3-103   1/1       Running   0          21s
kube-system   kube-dns-86f4d74b45-d2cvp         3/3       Running   0          1m
kube-system   kube-flannel-ds-vgmtv             1/1       Running   0          1m
kube-system   kube-proxy-n8bzn                  1/1       Running   0          1m
kube-system   kube-scheduler-c-3-103            1/1       Running   0          8s
[root@c-3-103 k8s]# 
[root@c-3-103 k8s]# kubectl get nodes
NAME      STATUS    ROLES     AGE       VERSION
c-3-103   Ready     master    1m        v1.10.13
```

# 06、 join work nodes
```
[root@c-3-104 ~]# kubeadm join 192.168.3.103:6443 --token ru8ivw.7601bgz1gypd2zup --discovery-token-ca-cert-hash sha256:37a26455b5fa97c8ac3e4e80e146b275547ad5fb13535c52276
a4f46984850c4[preflight] Running pre-flight checks.
	[WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.06.3-ce. Max validated version: 17.03
	[WARNING FileExisting-crictl]: crictl not found in system path
Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl
[discovery] Trying to connect to API Server "192.168.3.103:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.3.103:6443"
[discovery] Requesting info from "https://192.168.3.103:6443" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "192.168.3.103:6443"
[discovery] Successfully established connection with API Server "192.168.3.103:6443"

This node has joined the cluster:
* Certificate signing request was sent to master and a response
  was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
[root@c-3-104 ~]#

#check again
[root@c-3-103 k8s]# kubectl get pods --all-namespaces
NAMESPACE     NAME                              READY     STATUS    RESTARTS   AGE
kube-system   etcd-c-3-103                      1/1       Running   0          4m
kube-system   kube-apiserver-c-3-103            1/1       Running   0          4m
kube-system   kube-controller-manager-c-3-103   1/1       Running   0          3m
kube-system   kube-dns-86f4d74b45-d2cvp         3/3       Running   0          4m
kube-system   kube-flannel-ds-vgmtv             1/1       Running   0          4m
kube-system   kube-flannel-ds-vj2cd             1/1       Running   0          20s
kube-system   kube-proxy-58kbg                  1/1       Running   0          20s
kube-system   kube-proxy-n8bzn                  1/1       Running   0          4m
kube-system   kube-scheduler-c-3-103            1/1       Running   0          3m
[root@c-3-103 k8s]#

[root@c-3-103 k8s]# kubectl get nodes
NAME      STATUS    ROLES     AGE       VERSION
c-3-103   Ready     master    5m        v1.10.13
c-3-104   Ready     <none>    23s       v1.10.13
[root@c-3-103 k8s]# 
```

# 07、kubernetes dashboard
```
kubectl apply -f kubernetes-dashboard-http.yaml
kubectl apply -f admin-role.yaml
kubectl apply -f kubernetes-dashboard-admin.rbac.yaml
```
#useing  
http://192.168.3.103:31000/


# error
[link](https://www.cnblogs.com/caibao666/p/11664726.html)

#flannel pod
```
[root@c-3-103 k8s]# kubectl get pods --all-namespaces
NAMESPACE     NAME                              READY     STATUS              RESTARTS   AGE
kube-system   etcd-c-3-103                      1/1       Running             0          11m
kube-system   kube-apiserver-c-3-103            1/1       Running             0          11m
kube-system   kube-controller-manager-c-3-103   1/1       Running             0          10m
kube-system   kube-dns-86f4d74b45-798rx         0/3       ContainerCreating   0          11m
kube-system   kube-flannel-ds-zqxp7             0/1       CrashLoopBackOff    5          5m
kube-system   kube-proxy-7fdhz                  1/1       Running             0          11m
kube-system   kube-scheduler-c-3-103            1/1       Running             0          10m
[root@c-3-103 k8s]# 
[root@c-3-103 k8s]# kubectl -n kube-system logs  pods/kube-flannel-ds-zqxp7
I0626 06:49:51.970836       1 main.go:474] Determining IP address of default interface
I0626 06:49:51.971487       1 main.go:487] Using interface with name eth0 and address 192.168.3.103
I0626 06:49:51.971519       1 main.go:504] Defaulting external address to interface address (192.168.3.103)
I0626 06:49:51.986041       1 kube.go:130] Waiting 10m0s for node controller to sync
I0626 06:49:51.986088       1 kube.go:283] Starting kube subnet manager
I0626 06:49:52.986671       1 kube.go:137] Node controller sync successful
I0626 06:49:52.986777       1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - c-3-103
I0626 06:49:52.986791       1 main.go:237] Installing signal handlers
I0626 06:49:52.987028       1 main.go:352] Found network config - Backend type: vxlan
I0626 06:49:52.987108       1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false
E0626 06:49:52.987734       1 main.go:279] Error registering network: failed to acquire lease: node "c-3-103" pod cidr not assigned
I0626 06:49:52.987837       1 main.go:332] Stopping shutdownHandler...
[root@c-3-103 k8s]# 
```

#问题原因
```
Kubeadm Init的时候，没有增加 --pod-network-cidr 10.244.0.0/16参数或者kube-flannel.yml如果yml中的"Network": "10.244.0.0/16"和--pod-network-cidr不一样，所以，修改yml文件中的Network为相同网段后即可
```

#kubeadm join  
[link](https://blog.csdn.net/qq_34857250/article/details/82562514)

```
[root@c-3-104 ~]# kubeadm join 192.168.3.103:6443 --token ru8ivw.7601bgz1gypd2zup --discovery-token-ca-cert-hash sha256:37a26455b5fa97c8ac3e4e80e146b275547ad5fb13535c52276
a4f46984850c4[preflight] Running pre-flight checks.
	[WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.06.3-ce. Max validated version: 17.03
[preflight] Some fatal errors occurred:
	[ERROR CRI]: unable to check if the container runtime at "/var/run/dockershim.sock" is running: exit status 1
[preflight] If you kn
```
#问题解决
```
[root@c-3-104 ~]# rm -f /usr/bin/crictl
```